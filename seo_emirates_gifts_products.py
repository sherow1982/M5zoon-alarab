#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ø­Ù‚Ù† Ø§Ù„Ø³ÙƒÙŠÙ…Ø§ ÙˆØ§Ù„Ù…ÙŠØªØ§ ØªØ§Øº Ù„ÙƒÙ„ ØµÙØ­Ø§Øª Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ products Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BeautifulSoup.
Ù‡Ø°Ø§ Ø§Ù„Ø³ÙƒØ±Ø¨Øª ÙŠÙ‚Ø±Ø£ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ù† Ù…Ù„Ù JSON Ø®Ø§Ø±Ø¬ÙŠ Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ù…Ø±ÙˆÙ†Ø©.
"""

import sys
import re
import json
from pathlib import Path
from datetime import datetime, timedelta

try:
    from bs4 import BeautifulSoup
except ImportError:
    print("Ù…ÙƒØªØ¨Ø© BeautifulSoup ØºÙŠØ± Ù…Ø«Ø¨ØªØ©. ÙŠØ±Ø¬Ù‰ ØªØ«Ø¨ÙŠØªÙ‡Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù…: pip install beautifulsoup4")
    sys.exit(1)

# --- Ø«ÙˆØ§Ø¨Øª ---
DEFAULT_PRODUCT_TITLE = "Ù…Ù†ØªØ¬ Ù…Ù…ÙŠØ² Ù…Ù† Ù‡Ø¯Ø§ÙŠØ§ Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª"
MAIN_IMAGE_SELECTOR = "img.product-image, .main-product-image img, #product-image" # CSS selector for main product image
PRICE_REGEX_PATTERNS = [
    r'([\d,]+(?:\.\d{1,2})?)\s*AED',
    r'([\d,]+(?:\.\d{1,2})?)\s*Ø¯\.Ø¥',
    r'([\d,]+(?:\.\d{1,2})?)\s*Ø¯Ø±Ù‡Ù…',
]


def get_script_and_root_dirs():
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©."""
    script_dir = Path(__file__).parent.resolve()
    root_dir = script_dir.parent
    return script_dir, root_dir


def setup_logging():
    """Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« (Logging)."""
    # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ØªØ³Ø¬ÙŠÙ„ Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„Ø§Ù‹ Ù‡Ù†Ø§ ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„
    sys.exit(1)


def load_config(config_path):
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ù† Ù…Ù„Ù JSON."""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø£Ùˆ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª {config_path}: {e}")
        sys.exit(1)


def extract_product_info(soup, config):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù†ØªØ¬ Ù…Ù† ÙƒØ§Ø¦Ù† BeautifulSoup."""
    info = {}
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
    title_tag = soup.find('title')
    h1_tag = soup.find('h1')
    if title_tag and title_tag.string:
        info['title'] = title_tag.string.split('|')[0].strip()
    elif h1_tag and h1_tag.string:
        info['title'] = h1_tag.string.strip()
    else:
        info['title'] = DEFAULT_PRODUCT_TITLE
        print("   âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¹Ù†ÙˆØ§Ù†ØŒ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚ÙŠÙ…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©.")

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±Ø©
    img_tag = soup.select_one(MAIN_IMAGE_SELECTOR) or soup.find('img')
    if img_tag and img_tag.get('src'):
        src = img_tag['src']
        info['image'] = src if src.startswith('http') else f"{config['base_url']}{src if src.startswith('/') else '/' + src}"
    else:
        info['image'] = f"{config['base_url']}{config['default_image']}"
        print("   âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØµÙˆØ±Ø©ØŒ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… ØµÙˆØ±Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©.")
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø³Ø¹Ø±
    html_text = soup.get_text()
    price = 0.0
    for pattern in PRICE_REGEX_PATTERNS:
        m = re.search(pattern, html_text, re.IGNORECASE)
        if m:
            try:
                # Ø¥Ø²Ø§Ù„Ø© ÙØ§ØµÙ„ Ø§Ù„Ø¢Ù„Ø§Ù (,) Ø«Ù… ØªØ­ÙˆÙŠÙ„ ÙØ§ØµÙ„ Ø§Ù„Ø¹Ø´Ø±ÙŠØ© (Ø¥Ø°Ø§ ÙƒØ§Ù† ,) Ø¥Ù„Ù‰ .
                price_str = m.group(1).replace(',', '')
                price = float(price_str)
                break
            except ValueError:
                # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø¯ÙŠÙ„Ø© Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ ØµÙŠØºØ© Ù…Ø«Ù„ 1,23
                try:
                    price = float(m.group(1).replace(',', '.'))
                    break
                except ValueError:
                    continue
                continue
    info['price'] = price
    if price == 0.0:
        print("   âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø³Ø¹Ø±ØŒ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‚ÙŠÙ…Ø© 0.0.")

    return info


def build_product_url(file_path, base_url):
    """Ø¨Ù†Ø§Ø¡ Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù†ØªØ¬ Ø§Ù„ÙƒØ§Ù…Ù„."""
    return f"{base_url}/products/{file_path.name}"


def create_product_schema(info, url, config):
    """Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒÙŠÙ…Ø§ Ø§Ù„Ù…Ù†ØªØ¬ Ø¨ØµÙŠØºØ© JSON-LD."""
    price_valid_until = (datetime.now() + timedelta(days=365)).strftime('%Y-%m-%d')
    schema = {
        "@context": "https://schema.org/",
        "@type": "Product",
        "name": info['title'],
        "image": [info['image']],
        "description": f"{info['title']} - Ù‡Ø¯Ø§ÙŠØ§ ÙØ±ÙŠØ¯Ø© Ù…Ù† {config['brand_name']} Ù…Ø¹ ØªÙˆØµÙŠÙ„ Ø³Ø±ÙŠØ¹",
        "brand": {
            "@type": "Brand",
            "name": config['brand_name']
        },
        "offers": {
            "@type": "Offer",
            "url": url,
            "priceCurrency": config['product_defaults']['currency'],
            "price": str(info['price']),
            "priceValidUntil": price_valid_until,
            "itemCondition": config['product_defaults']['condition'],
            "availability": config['product_defaults']['availability'],
            "seller": {
                "@type": "Organization",
                "name": config['brand_name']
            }
        }
    }
    return schema


def create_local_business_schema(config):
    """Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒÙŠÙ…Ø§ Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„ØªØ¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø­Ù„ÙŠ Ø¨ØµÙŠØºØ© JSON-LD."""
    details = config['business_details']
    schema = {
        "@context": "https://schema.org",
        "@type": "LocalBusiness",
        "name": details['name'],
        "image": f"{config['base_url']}{config['default_image']}",
        "url": config['base_url'],
        "telephone": details['telephone'],
        "address": details['address'],
        "geo": details['geo'],
        "openingHours": details['openingHours'],
        "priceRange": details['priceRange']
    }
    return schema


def update_meta_tag(soup, name, content, is_property=False):
    """ØªØ­Ø¯ÙŠØ« Ø£Ùˆ Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙŠØªØ§ ØªØ§Øº."""
    attr = 'property' if is_property else 'name'
    tag = soup.find('meta', {attr: name})
    if not tag:
        tag = soup.new_tag('meta')
        tag[attr] = name
        soup.head.append(tag)
    tag['content'] = content


def inject_seo(soup, info, url, config):
    """Ø­Ù‚Ù† Ø¨ÙŠØ§Ù†Ø§Øª SEO Ùˆ JSON-LD ÙÙŠ ÙƒØ§Ø¦Ù† BeautifulSoup."""
    if not soup.head:
        soup.head = soup.new_tag('head')
        soup.body.insert_before(soup.head)

    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø³ÙƒÙŠÙ…Ø§ ÙˆØ§Ù„Ù…ÙŠØªØ§ ØªØ§Øº Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø§Ù„ØªÙŠ ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§
    for tag in soup.find_all(lambda t: t.string and 'Auto-generated' in t.string):
        tag.find_parent().decompose()
    for script in soup.find_all('script', type='application/ld+json'):
        script.decompose()

    # ØªØ­Ø¯ÙŠØ« Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØµÙØ­Ø©
    if soup.title:
        soup.title.string = f"{info['title']} - {config['brand_name']} | Ù‡Ø¯Ø§ÙŠØ§ ÙØ±ÙŠØ¯Ø© ÙˆØ¹Ø±ÙˆØ¶ Ø­ØµØ±ÙŠØ©"
    else:
        new_title = soup.new_tag('title')
        new_title.string = f"{info['title']} - {config['brand_name']}"
        soup.head.append(new_title)

    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…ÙŠØªØ§ ØªØ§Øº
    desc = f"{info['title']} - Ù‡Ø¯Ø§ÙŠØ§ ÙØ±ÙŠØ¯Ø© Ù…Ù† {config['brand_name']} Ù…Ø¹ ØªÙˆØµÙŠÙ„ Ø³Ø±ÙŠØ¹ Ù„ÙƒÙ„ Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª"
    desc = (desc[:152] + '...') if len(desc) > 155 else desc
    
    update_meta_tag(soup, 'description', desc)
    update_meta_tag(soup, 'keywords', f"{info['title']}, {config['brand_name']}, Ù‡Ø¯Ø§ÙŠØ§, ØªØ³ÙˆÙ‚ Ø§ÙˆÙ†Ù„Ø§ÙŠÙ†, Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª")
    update_meta_tag(soup, 'og:title', f"{info['title']} - {config['brand_name']}", is_property=True)
    update_meta_tag(soup, 'og:description', desc, is_property=True)
    update_meta_tag(soup, 'og:image', info['image'], is_property=True)
    update_meta_tag(soup, 'og:url', url, is_property=True)
    update_meta_tag(soup, 'og:site_name', config['brand_name'], is_property=True)
    update_meta_tag(soup, 'twitter:card', 'summary_large_image')
    update_meta_tag(soup, 'twitter:title', f"{info['title']} - {config['brand_name']}")
    update_meta_tag(soup, 'twitter:description', desc)
    update_meta_tag(soup, 'twitter:image', info['image'])

    # Ø¥Ø¶Ø§ÙØ© Canonical URL
    if not soup.find('link', rel='canonical'):
        canonical_tag = soup.new_tag('link', rel='canonical', href=url)
        soup.head.append(canonical_tag)

    # Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ­Ù‚Ù† Ø§Ù„Ø³ÙƒÙŠÙ…Ø§
    product_schema = create_product_schema(info, url, config)
    local_schema = create_local_business_schema(config)

    product_script = soup.new_tag('script', type='application/ld+json')
    product_script.string = json.dumps(product_schema, ensure_ascii=False, indent=2)
    
    local_script = soup.new_tag('script', type='application/ld+json')
    local_script.string = json.dumps(local_schema, ensure_ascii=False, indent=2)

    soup.head.append(soup.new_string("\n<!-- Auto-generated SEO and Schema -->\n"))
    soup.head.append(product_script)
    soup.head.append("\n")
    soup.head.append(local_script)
    soup.head.append("\n")


def process_file(file_path, config):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù HTML ÙˆØ§Ø­Ø¯."""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            soup = BeautifulSoup(f, 'html.parser')

        product_info = extract_product_info(soup, config)
        product_url = build_product_url(file_path, config['base_url'])
        
        inject_seo(soup, product_info, product_url, config)

        with open(file_path, "w", encoding="utf-8") as f:
            f.write(str(soup.prettify(formatter='html5')))

        print(f"   âœ… {file_path.name}")
        return True
    except Exception as e:
        print(f"   âŒ {file_path.name}: {e}")
        return False


def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙƒØ±Ø¨Øª."""
    print("\n" + "="*70)
    print("ğŸ Ø³ÙƒØ±Ø¨Øª Ø³ÙƒÙŠÙ…Ø§ ÙˆSEO Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ø«Ø§Ø¨ØªØ© ÙÙŠ emirates-gifts ğŸ")
    print("="*70 + "\n")

    script_dir, root_dir = get_script_and_root_dirs()
    config = load_config(script_dir / "seo_config.json")

    products_dir = root_dir / "products"
    if not products_dir.exists():
        print(f"âŒ Ù…Ø¬Ù„Ø¯ products ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ: {products_dir.resolve()}")
        sys.exit(1)

    html_files = sorted(list(products_dir.glob("*.html")))
    if not html_files:
        print("âŒ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø£ÙŠ Ù…Ù„ÙØ§Øª HTML Ø¯Ø§Ø®Ù„ products/\n")
        sys.exit(1)

    print(f"ğŸ“¦ {len(html_files)} ØµÙØ­Ø© Ù…Ù†ØªØ¬ ÙÙŠ products/\n")
    ok = 0
    fail = 0
    for i, fp in enumerate(html_files, 1):
        print(f"[{i}/{len(html_files)}] Ù…Ø¹Ø§Ù„Ø¬Ø©: {fp.name} ...", end=' ')
        if process_file(fp, config):
            ok += 1
        else:
            fail += 1

    print("\n" + "="*70)
    print(f"âœ… Ù†Ø¬Ø­: {ok} Ù…Ù„Ù")
    print(f"âŒ ÙØ´Ù„: {fail} Ù…Ù„Ù")
    if html_files:
        print(f"ğŸ“ˆ Ù†Ø³Ø¨Ø© Ø§Ù„Ù†Ø¬Ø§Ø­: {(ok/len(html_files)*100):.1f}%")
    print("="*70)
    print("\nâœ¨ Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªÙ†ÙÙŠØ° Ø¨Ù†Ø¬Ø§Ø­! Ø§Ù„Ø¢Ù† ÙƒÙ„ ØµÙØ­Ø© Ù…Ù†ØªØ¬ ØªØ­ØªÙˆÙŠ Ø¹Ù†Ø§ØµØ± Ø³ÙƒÙŠÙ…Ø§ JSON-LD Ø­Ù‚ÙŠÙ‚ÙŠØ©\n")


if __name__ == "__main__":
    main()
